{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install box2d-py\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the BipedalWalker-v3 environment\n",
        "env = gym.make(\"BipedalWalker-v3\", hardcore=True)\n",
        "\n",
        "# Dynamics Value Iteration\n",
        "\n",
        "def dynamics_value_iteration(env, discount_factor=0.9, theta=1e-9):\n",
        "    # Get the number of states and actions\n",
        "    nS = env.observation_space.shape[0]\n",
        "    nA = env.action_space.shape[0]\n",
        "\n",
        "    # Initialize the value function\n",
        "    V = np.zeros(nS)\n",
        "\n",
        "    # Initialize the list to store the reward curve\n",
        "    reward_curve = []\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        total_reward = 0\n",
        "\n",
        "        # Iterate over all states\n",
        "        for s in range(nS):\n",
        "            v = V[s]\n",
        "            q_values = np.zeros(nA)\n",
        "\n",
        "            # Compute Q-value for each action\n",
        "            for a in range(nA):\n",
        "                next_state, reward, _, _ = env.step(a)\n",
        "                total_reward += reward\n",
        "                q_values[a] = reward + discount_factor * V[next_state]\n",
        "\n",
        "            # Update the value function\n",
        "            V[s] = np.max(q_values)\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "\n",
        "        # Store the total reward in each iteration\n",
        "        reward_curve.append(total_reward)\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V, reward_curve\n",
        "\n",
        "# Approximate Dynamics Programming using Polynomial Features\n",
        "\n",
        "def approximate_dynamics_programming(env, discount_factor=0.9):\n",
        "    # Get the number of states and actions\n",
        "    nS = env.observation_space.shape[0]\n",
        "    nA = env.action_space.shape[0]\n",
        "\n",
        "    # Generate features for all states\n",
        "    all_states = env.observation_space.sample().reshape(1, -1)\n",
        "    features = generate_features(all_states)\n",
        "\n",
        "    # Initialize the value function\n",
        "    V = np.zeros(nS)\n",
        "\n",
        "    # Initialize the list to store the reward curve\n",
        "    reward_curve = []\n",
        "\n",
        "    while True:\n",
        "        # Generate targets for each state\n",
        "        targets = np.zeros(nS)\n",
        "        total_reward = 0\n",
        "\n",
        "        # Iterate over all states\n",
        "        for s in range(nS):\n",
        "            q_values = np.zeros(nA)\n",
        "\n",
        "            # Compute Q-value for each action\n",
        "            for a in range(nA):\n",
        "                next_state, reward, _, _ = env.step(a)\n",
        "                total_reward += reward\n",
        "                features_next = generate_features(next_state.reshape(1, -1))\n",
        "                q_values[a] = reward + discount_factor * np.max(np.dot(features_next, weights))\n",
        "\n",
        "            # Update the targets\n",
        "            targets[s] = np.max(q_values)\n",
        "\n",
        "        # Fit a linear regression model\n",
        "        regression_model = LinearRegression()\n",
        "        regression_model.fit(features, targets)\n",
        "        weights = regression_model.coef_\n",
        "\n",
        "        # Store the total reward in each iteration\n",
        "        reward_curve.append(total_reward)\n",
        "\n",
        "        if np.allclose(V, targets):\n",
        "            break\n",
        "\n",
        "    return V, reward_curve\n",
        "\n",
        "# Deep Function Approximation of Dynamics Programming\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def deep_function_approximation(env, discount_factor=0.9, learning_rate=0.01, num_epochs=100):\n",
        "    # Get the number of states and actions\n",
        "    nS = env.observation_space.shape[0]\n",
        "    nA = env.action_space.shape[0]\n",
        "\n",
        "    # Generate features for all states\n",
        "    all_states = env.observation_space.sample().reshape(1, -1)\n",
        "    features = generate_features(all_states)\n",
        "\n",
        "    # Create the value network\n",
        "    input_dim = features.shape[1]\n",
        "    output_dim = nA\n",
        "    value_net = ValueNet(input_dim, output_dim)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(value_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Convert features and values to tensors\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "    values_tensor = torch.tensor(approximate_values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Initialize the list to store the reward curve\n",
        "    reward_curve = []\n",
        "\n",
        "    # Train the value network\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        values_pred = value_net(features_tensor)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(values_pred, values_tensor)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the total reward\n",
        "        total_reward = 0\n",
        "        for _ in range(10):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = value_net(torch.tensor(generate_features(state.reshape(1, -1)), dtype=torch.float32)).argmax().item()\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "\n",
        "        # Store the total reward in each epoch\n",
        "        reward_curve.append(total_reward)\n",
        "\n",
        "    # Extract the values from the value network\n",
        "    with torch.no_grad():\n",
        "        values_pred = value_net(features_tensor).numpy().flatten()\n",
        "\n",
        "    return values_pred, reward_curve\n",
        "\n",
        "# Generate polynomial features for state representation\n",
        "def generate_features(states):\n",
        "    polynomial_features = PolynomialFeatures(degree=2)\n",
        "    features = polynomial_features.fit_transform(states)\n",
        "    return features\n",
        "\n",
        "# Apply dynamics value iteration on the BipedalWalker-v3 environment\n",
        "values_vi, reward_curve_vi = dynamics_value_iteration(env)\n",
        "\n",
        "# Apply approximate dynamics programming using polynomial features on the BipedalWalker-v3 environment\n",
        "values_adp, reward_curve_adp = approximate_dynamics_programming(env)\n",
        "\n",
        "# Apply deep function approximation of dynamics programming on the BipedalWalker-v3 environment\n",
        "values_dfa, reward_curve_dfa = deep_function_approximation(env)\n",
        "\n",
        "# Plot the reward curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(reward_curve_vi, label='Dynamics Value Iteration')\n",
        "plt.plot(reward_curve_adp, label='Approximate Dynamics Programming')\n",
        "plt.plot(reward_curve_dfa, label='Deep Function Approximation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "91Z-mMiwlH7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOIZet7kkQ_t"
      },
      "outputs": [],
      "source": [
        "!pip install gym\n",
        "!pip install box2d-py\n",
        "import gym\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "# Dynamics Value Iteration Algorithm\n",
        "def dynamics_value_iteration(env, gamma=0.99, max_iterations=1000, epsilon=1e-6):\n",
        "    # Initialize value function and policy\n",
        "    value_function = [0.0] * env.observation_space.shape[0]\n",
        "\n",
        "    # Perform value iteration\n",
        "    for iteration in range(max_iterations):\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.shape[0]):\n",
        "            old_value = value_function[state]\n",
        "            action_values = []\n",
        "            for action in range(env.action_space.shape[0]):\n",
        "                next_state_rewards = []\n",
        "                for next_state in range(env.observation_space.shape[0]):\n",
        "                    next_state_rewards.append(env.env.calc_reward(state, action, next_state))\n",
        "                action_values.append(sum(next_state_rewards) + gamma * value_function[next_state])\n",
        "            value_function[state] = max(action_values)\n",
        "            delta = max(delta, abs(old_value - value_function[state]))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "\n",
        "    return value_function\n",
        "\n",
        "# Approximate Dynamics Programming with Polynomial Features\n",
        "def approximate_dynamics_programming(env, degree=2):\n",
        "    # Generate polynomial features for each state\n",
        "    states = []\n",
        "    for state in range(env.observation_space.shape[0]):\n",
        "        states.append(env.env.get_state(state))\n",
        "\n",
        "    # Generate polynomial features\n",
        "    features = []\n",
        "    for state in states:\n",
        "        state_features = []\n",
        "        for d in range(1, degree + 1):\n",
        "            state_features += [state[i] ** d for i in range(len(state))]\n",
        "        features.append(state_features)\n",
        "\n",
        "    # Perform linear regression to approximate dynamics\n",
        "    dynamics_model = LinearRegression()\n",
        "    dynamics_model.fit(features, states)\n",
        "\n",
        "    return dynamics_model\n",
        "\n",
        "# Deep Function Approximation of Dynamics Programming (using PyTorch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DynamicsModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super(DynamicsModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def deep_function_approximation(env, hidden_dim=64, learning_rate=0.001, num_epochs=100):\n",
        "    # Prepare the data\n",
        "    data = []\n",
        "    for state in range(env.observation_space.shape[0]):\n",
        "        for action in range(env.action_space.shape[0]):\n",
        "            for next_state in range(env.observation_space.shape[0]):\n",
        "                reward = env.env.calc_reward(state, action, next_state)\n",
        "                data.append((state, action, next_state, reward))\n",
        "\n",
        "    # Define the model and optimizer\n",
        "    model = DynamicsModel(env.observation_space.shape[0] + env.action_space.shape[0], env.observation_space.shape[0], hidden_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for state, action, next_state, reward in data:\n",
        "            input_data = torch.tensor([state + action], dtype=torch.float32)\n",
        "            target = torch.tensor([next_state], dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(data)\n",
        "        print(f\"Epoch: {epoch+1}, Loss: {average_loss}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Apply the Dynamics Value Iteration Algorithm\n",
        "value_function = dynamics_value_iteration(env)\n",
        "\n",
        "# Apply Approximate Dynamics Programming with Polynomial Features\n",
        "degree = 2\n",
        "dynamics_model = approximate_dynamics_programming(env, degree)\n",
        "\n",
        "# Apply Deep Function Approximation of Dynamics Programming\n",
        "hidden_dim = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "deep_model = deep_function_approximation(env, hidden_dim, learning_rate, num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziSDAamHkXHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7Tv3QKctR48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXiVrusItSmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}